{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the phoenix collector endpoint. Commonly http://localhost:6060 \n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://0.0.0.0:6006/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "client = px.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current dataset version\n",
    "dataset = client.get_dataset(id=\"RGF0YXNldDox\", version_id=\"RGF0YXNldFZlcnNpb246MQ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.types import Example\n",
    "# Define your task\n",
    "# Typically should be an LLM call or a call to your application\n",
    "def my_task(example: Example) -> str:\n",
    "    # This is just an example of how to return a JSON serializable value\n",
    "    return f\"Hello {example.input[\"person\"]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluator. This just an example.\n",
    "def exact_match(input, output) -> float:\n",
    "    return 1.0 if output is f\"Hello {input}\" else 0.0\n",
    "\n",
    "# Store the evaluators for later use\n",
    "evaluators = [exact_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸŒ!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Experiment started.\n",
      "ğŸ“º View dataset experiments: http://0.0.0.0:6006/datasets/RGF0YXNldDox/experiments\n",
      "ğŸ”— View this experiment: http://0.0.0.0:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |          | 0/1 (0.0%) | â³ 00:00<? | ?it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTraceback (most recent call last):\n",
      "  File \"/Users/cam/Projects/new_env/lib/python3.12/site-packages/phoenix/experiments/functions.py\", line 252, in sync_run_experiment\n",
      "    _output = task(*bound_task_args.args, **bound_task_args.kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/n1/jtdxtw5s58s3jbz0r4dgl_dw0000gn/T/ipykernel_45105/1508798117.py\", line 6, in my_task\n",
      "    return f\"Hello {example.input[\"person\"]}\"\n",
      "                    ~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'person'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "RuntimeError: task failed for example id 'RGF0YXNldEV4YW1wbGU6MQ==', repetition 1\n",
      "\u001b[0m\n",
      "âœ… Task runs completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/Projects/new_env/lib/python3.12/site-packages/phoenix/utilities/client.py:36: UserWarning: âš ï¸âš ï¸ The Phoenix server (5.12.0) and client (4.33.2) versions are severely mismatched. Upgrade  either the client or server to ensure API compatibility âš ï¸âš ï¸\n",
      "  warnings.warn(\n",
      "ğŸŒ!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/Projects/new_env/lib/python3.12/site-packages/phoenix/utilities/client.py:36: UserWarning: âš ï¸âš ï¸ The Phoenix server (5.12.0) and client (4.33.2) versions are severely mismatched. Upgrade  either the client or server to ensure API compatibility âš ï¸âš ï¸\n",
      "  warnings.warn(\n",
      "running experiment evaluations |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 (100.0%) | â³ 00:00<00:00 | 31.65it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— View this experiment: http://0.0.0.0:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n",
      "\n",
      "Experiment Summary (12/02/24 09:35 AM -0800)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  exact_match  1         1        0.0\n",
      "\n",
      "Tasks Summary (12/02/24 09:35 AM -0800)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors           top_error\n",
      "0           1       1         1  KeyError('person')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run an experiment\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(dataset, my_task, evaluators=evaluators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
