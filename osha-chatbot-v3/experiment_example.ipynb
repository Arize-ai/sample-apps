{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camyoung/Documents/Projects/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.experiments.evaluators.base import (\n",
    "    EvaluationResult,\n",
    "    Evaluator,\n",
    ")\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up environment variables for Azure OpenAI and Arize\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"your_azure_endpoint\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n",
    "os.environ[\"ARIZE_SPACE_ID\"] = \"your_arize_space_id\"\n",
    "os.environ[\"ARIZE_API_KEY\"] = \"your_arize_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:All required environment variables are set\n",
      "INFO:__main__:Current Azure OpenAI deployment: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.experiments.evaluators.base import (\n",
    "    EvaluationResult,\n",
    "    Evaluator,\n",
    ")\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Verify that required environment variables are present\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_ENDPOINT\", \n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\",\n",
    "    \"ARIZE_SPACE_ID\",\n",
    "    \"ARIZE_API_KEY\",\n",
    "    \"ARIZE_MODEL_ID\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "if missing_vars:\n",
    "    logger.warning(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "else:\n",
    "    logger.info(\"All required environment variables are set\")\n",
    "\n",
    "# Print available deployments for reference\n",
    "if \"AZURE_OPENAI_DEPLOYMENT\" in os.environ:\n",
    "    logger.info(f\"Current Azure OpenAI deployment: {os.environ['AZURE_OPENAI_DEPLOYMENT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_openai_client(deployment_name):\n",
    "    \"\"\"\n",
    "    Creates an Azure OpenAI client for a specific deployment\n",
    "    \n",
    "    Args:\n",
    "        deployment_name (str): The Azure OpenAI deployment name\n",
    "        \n",
    "    Returns:\n",
    "        AzureOpenAI: Configured Azure OpenAI client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to use API key if available\n",
    "        api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "        if api_key:\n",
    "            logger.info(f\"Using API key authentication for {deployment_name}\")\n",
    "            client = AzureOpenAI(\n",
    "                api_key=api_key,\n",
    "                azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "            )\n",
    "        else:\n",
    "            # Use DefaultAzureCredential\n",
    "            logger.info(f\"Using DefaultAzureCredential for {deployment_name}\")\n",
    "            default_credential = DefaultAzureCredential()\n",
    "            client = AzureOpenAI(\n",
    "                azure_ad_token_provider=default_credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
    "                azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "            )\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Azure OpenAI client: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, model_client, deployment_name):\n",
    "    \"\"\"\n",
    "    Generate a response using your existing classification and query logic\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query\n",
    "        model_client (AzureOpenAI): The Azure OpenAI client\n",
    "        deployment_name (str): The deployment name\n",
    "        \n",
    "    Returns:\n",
    "        dict: Response with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Classification prompt from your config\n",
    "        classification_prompt = \"\"\"You are a query classifier for Assurant's 10-K reports and risk assessment application. \n",
    "        Analyze the following query and respond with a JSON object containing two fields:\n",
    "        1. 'category': Must be exactly one of: \"assurant_10k\", \"risk_assessment\", or \"out_of_scope\"\n",
    "        2. 'confidence': A number between 0 and 1 indicating your confidence in the classification\n",
    "        \n",
    "        Query: {query}\n",
    "        \n",
    "        Respond with ONLY a valid JSON object in this exact format:\n",
    "        {{\"category\": \"<category>\", \"confidence\": <confidence>}}\"\"\"\n",
    "        \n",
    "        # Get classification\n",
    "        classification_response = model_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": classification_prompt.format(query=query)},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        classification_text = classification_response.choices[0].message.content\n",
    "        classification = json.loads(classification_text.strip())\n",
    "        \n",
    "        # Define RAG prompt for responses\n",
    "        rag_prompt = \"\"\"You are a financial analyst specializing in insurance companies with expert knowledge of Assurant's recent 10-K reports. Provide a clear, accurate answer based on the context.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        When applicable, cite specific sections, page numbers, or fiscal years from the 10-K reports.\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = model_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": rag_prompt.format(query=query)},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"category\": classification[\"category\"],\n",
    "            \"confidence\": classification[\"confidence\"],\n",
    "            \"model\": deployment_name\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating response: {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"category\": \"error\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"model\": deployment_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_task(deployment_name):\n",
    "    \"\"\"\n",
    "    Create an Arize experiment task for a specific model deployment\n",
    "    \n",
    "    Args:\n",
    "        deployment_name (str): The Azure OpenAI deployment name\n",
    "        \n",
    "    Returns:\n",
    "        ExperimentTask: Configured experiment task\n",
    "    \"\"\"\n",
    "    client = create_azure_openai_client(deployment_name)\n",
    "    \n",
    "    def task_function(df):\n",
    "        \"\"\"Process each row in the dataset\"\"\"\n",
    "        results = []\n",
    "        for _, row in df.iterrows():\n",
    "            query = row[\"query\"]\n",
    "            response_data = generate_response(query, client, deployment_name)\n",
    "            results.append(response_data)\n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    return ExperimentTask(\n",
    "        name=f\"azure-openai-{deployment_name}\",\n",
    "        function=task_function\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Phoenix evaluations\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "from arize.experimental.datasets.experiments.evaluators.base import (\n",
    "    EvaluationResult,\n",
    "    Evaluator,\n",
    ")\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio in Jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define custom hallucination evaluation prompt\n",
    "HALLUCINATION_PROMPT = \"\"\"In this task, you will be presented with a query, a reference text and an answer. The answer is\n",
    "generated to the question based on the reference text. The answer may contain false information. You\n",
    "must use the reference text to determine if the answer to the question contains false information,\n",
    "if the answer is a hallucination of facts. Your objective is to determine whether the answer text\n",
    "contains factual information and is not a hallucination. A 'hallucination' refers to\n",
    "an answer that is not based on the reference text or assumes information that is not available in\n",
    "the reference text. Your response should be a single word: either \"factual\" or \"hallucinated\", and\n",
    "it should not include any other text or characters. \"hallucinated\" indicates that the answer\n",
    "provides factually inaccurate information to the query based on the reference text. \"factual\"\n",
    "indicates that the answer to the question is correct relative to the reference text, and does not\n",
    "contain made up information. Please read the query and reference text carefully before determining\n",
    "your response.\n",
    "\n",
    "    # Query: {query}\n",
    "    # Reference text: {reference}\n",
    "    # Answer: {response}\n",
    "    Is the answer above factual or hallucinated based on the query and reference text?\"\"\"\n",
    "\n",
    "class HallucinationEvaluator(Evaluator):\n",
    "    def __init__(self, azure_client, deployment_name):\n",
    "        self.client = azure_client\n",
    "        self.deployment_name = deployment_name\n",
    "        self.rails = [\"hallucinated\", \"factual\"]\n",
    "        \n",
    "    async def async_llm_classify(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Create an OpenAIModel using the Azure client's credentials\n",
    "        azure_api_key = self.client._api_key if hasattr(self.client, '_api_key') else None\n",
    "        azure_endpoint = self.client._azure_endpoint if hasattr(self.client, '_azure_endpoint') else None\n",
    "        \n",
    "        # Create a model with the appropriate configuration\n",
    "        model = OpenAIModel(\n",
    "            model=self.deployment_name,\n",
    "            api_key=azure_api_key,\n",
    "            api_base=azure_endpoint,\n",
    "            api_type=\"azure\",\n",
    "            api_version=self.client._api_version if hasattr(self.client, '_api_version') else None\n",
    "        )\n",
    "        \n",
    "        result = await asyncio.to_thread(\n",
    "            llm_classify,\n",
    "            dataframe=dataframe,\n",
    "            model=model,\n",
    "            template=HALLUCINATION_PROMPT,\n",
    "            rails=self.rails,\n",
    "            run_sync=False,\n",
    "            concurrency=5,\n",
    "            provide_explanation=True\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def evaluate(self, output: str, dataset_row: dict, **kwargs) -> EvaluationResult:\n",
    "        # Build the dataframe\n",
    "        df = pd.DataFrame([{\n",
    "            \"query\": dataset_row.get(\"query\"),\n",
    "            \"response\": output\n",
    "        }])\n",
    "\n",
    "        # Run our async classification in a blocking manner\n",
    "        results = asyncio.run(self.async_llm_classify(df))\n",
    "        label = results['label'].iloc[0]\n",
    "        score = int(label)  # Convert string to integer\n",
    "\n",
    "        return EvaluationResult(\n",
    "            score=score,\n",
    "            label=label,\n",
    "            metadata={},\n",
    "            explanation=results['explanation'].iloc[0] if 'explanation' in results.columns else \"\"\n",
    "        )\n",
    "\n",
    "def create_evaluators(azure_client, evaluation_deployment):\n",
    "    \"\"\"\n",
    "    Create Phoenix-based hallucination evaluator for assessing model responses\n",
    "    \n",
    "    Args:\n",
    "        azure_client: The Azure OpenAI client to use for evaluations\n",
    "        evaluation_deployment: The deployment to use for evaluations\n",
    "        \n",
    "    Returns:\n",
    "        Evaluators: Configured evaluators\n",
    "    \"\"\"\n",
    "    hallucination = HallucinationEvaluator(azure_client, evaluation_deployment)\n",
    "    \n",
    "    return Evaluators([\n",
    "        (\"hallucination\", hallucination)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using API key authentication for gpt-4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Evaluators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m eval_client = create_azure_openai_client(evaluation_deployment)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create hallucination evaluator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m evaluators = \u001b[43mcreate_evaluators\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_deployment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m experiment_ids = {}\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Run experiment for each model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mcreate_evaluators\u001b[39m\u001b[34m(azure_client, evaluation_deployment)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03mCreate Phoenix-based hallucination evaluator for assessing model responses\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03m    Evaluators: Configured evaluators\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     96\u001b[39m hallucination = HallucinationEvaluator(azure_client, evaluation_deployment)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEvaluators\u001b[49m([\n\u001b[32m     99\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mhallucination\u001b[39m\u001b[33m\"\u001b[39m, hallucination)\n\u001b[32m    100\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'Evaluators' is not defined"
     ]
    }
   ],
   "source": [
    "# Main execution section\n",
    "if __name__ == \"__main__\":\n",
    "    # Define which models to evaluate\n",
    "    deployments = [\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4-turbo\",\n",
    "        \"gpt-35-turbo\"\n",
    "    ]\n",
    "    \n",
    "    # Set up experiment parameters\n",
    "    space_id = os.environ[\"ARIZE_SPACE_ID\"]\n",
    "    experiment_name = \"azure-openai-model-comparison\"\n",
    "    dataset_name = \"assurant-10k-queries\"\n",
    "    \n",
    "    # Define which model to use for evaluation\n",
    "    evaluation_deployment = \"gpt-4\"  # Using GPT-4 as the judge\n",
    "    \n",
    "    # Create Azure client for evaluations\n",
    "    eval_client = create_azure_openai_client(evaluation_deployment)\n",
    "    \n",
    "    # Create hallucination evaluator\n",
    "    evaluators = create_evaluators(eval_client, evaluation_deployment)\n",
    "    \n",
    "    experiment_ids = {}\n",
    "    \n",
    "    # Run experiment for each model\n",
    "    for deployment in deployments:\n",
    "        # Create the experiment task for this deployment\n",
    "        task = create_experiment_task(deployment)\n",
    "        \n",
    "        # Run the experiment directly\n",
    "        experiment_id, _ = run_experiment(\n",
    "            space_id=space_id,\n",
    "            experiment_name=f\"{experiment_name}-{deployment}\",\n",
    "            task=task,\n",
    "            dataset_name=dataset_name,\n",
    "            evaluators=evaluators,\n",
    "            dry_run=False,\n",
    "            concurrency=3\n",
    "        )\n",
    "        \n",
    "        experiment_ids[deployment] = experiment_id\n",
    "        logger.info(f\"Completed experiment for {deployment}: {experiment_id}\")\n",
    "    \n",
    "    # Print URLs for viewing results in Arize\n",
    "    print(f\"\\nView experiment results in Arize:\")\n",
    "    for deployment, exp_id in experiment_ids.items():\n",
    "        print(f\"  {deployment}: https://app.arize.com/spaces/{space_id}/experiments/{exp_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
